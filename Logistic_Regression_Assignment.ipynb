{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Q 1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "**Ans** - Logistic regression is a statistical model used primarily for binary classification tasks. It estimates the probability that a given input belongs to a particular category by applying a logistic function to a linear combination of the input features. This transformation squashes the output into the range [0, 1], which can then be interpreted as a probability.\n",
        "\n",
        "**Aspects of Logistic Regression**\n",
        "* Classification Focus: Logistic regression is designed for classification. It predicts probabilities that can be thresholded to decide class membership (e.g., yes/no, spam/not spam).\n",
        "\n",
        "* Sigmoid Function: The core of logistic regression is the logistic or sigmoid function, defined as\n",
        "\n",
        "      œÉ(z) = 1/(1+e‚Åª·∂ª)\n",
        "where 'z' is a linear combination of the input features. This function ensures the output is between 0 and 1.\n",
        "\n",
        "* Loss Function: Instead of minimizing the mean squared error, logistic regression typically uses the log loss, which is more appropriate for probability estimation and classification.\n",
        "\n",
        "* Estimation Technique: The parameters in logistic regression are usually estimated using maximum likelihood estimation, which finds the parameter values that maximize the likelihood of the observed data.\n",
        "\n",
        "**Differs from Linear Regression**\n",
        "* Purpose and Output:\n",
        "  * Linear Regression: Predicts a continuous outcome, such as price or temperature, by fitting a straight line to the data. The output is not constrained to any range.\n",
        "  * Logistic Regression: Predicts a probability for a categorical outcome, with outputs strictly between 0 and 1.\n",
        "\n",
        "* Modeling Relationship:\n",
        "  * Linear Regression: Assumes a linear relationship between the independent variables and the dependent variable.\n",
        "  * Logistic Regression: Assumes a linear relationship between the independent variables and the log-odds of the probability of the outcome.\n",
        "\n",
        "* Error Measurement:\n",
        "  * Linear Regression: Uses mean squared error to quantify the difference between the predicted continuous values and the actual values.\n",
        "  * Logistic Regression: Uses log loss to measure the performance of the model in classification tasks.\n",
        "\n",
        "* Interpretation:\n",
        "  * Linear Regression: The coefficients represent the change in the outcome variable for a one-unit change in the predictor.\n",
        "  * Logistic Regression: The coefficients represent the change in the log-odds of the outcome for a one-unit change in the predictor, which can be exponentiated to yield odds ratios."
      ],
      "metadata": {
        "id": "jkUhqR2iCsHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 2. What is the mathematical equation of Logistic Regression?\n",
        "**Ans** - The fundamental equation of logistic regression expresses the probability 'p' that an observation belongs to a particular class. It does this using the logistic function applied to a linear combination of input features. Mathematically, it is represented as:\n",
        "\n",
        "    p(x) = 1/(1+e‚Åª·∂ª)\n",
        "where\n",
        "z = Œ≤‚ÇÄ+Œ≤‚ÇÅx‚ÇÅ+Œ≤‚ÇÇx‚ÇÇ+‚ãØ+Œ≤‚Çñx‚Çñ\n",
        "Here:\n",
        "* p(x) is the predicted probability that the output is 1 given the input vector x.\n",
        "* Œ≤‚ÇÄ is the intercept.\n",
        "* Œ≤‚ÇÅ,Œ≤‚ÇÇ,‚Ä¶,Œ≤‚Çñ are the coefficients corresponding to the features x‚ÇÅ,x‚ÇÇ,‚Ä¶,x‚Çñ.\n",
        "\n",
        "An equivalent way to write this model is in terms of the log-odds (logit):\n",
        "\n",
        "    log‚Å°(p(x)/(1‚àíp(x))) = Œ≤‚ÇÄ+Œ≤‚ÇÅx‚ÇÅ+Œ≤‚ÇÇx‚ÇÇ+‚ãØ+Œ≤‚Çñx‚Çñ\n",
        "\n",
        "This equation shows that the log-odds of the outcome are modeled as a linear function of the input features."
      ],
      "metadata": {
        "id": "SOXDi7QRC2k7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "**Ans** - The sigmoid function is essential in logistic regression because it transforms the linear combination of inputs which can take any real value into a probability between 0 and 1. This mapping is crucial since logistic regression is used for classification tasks where outputs need to be interpreted as probabilities.\n",
        "\n",
        "**Reasons for Using Sigmoid Function**\n",
        "* Probability Mapping: The sigmoid function, defined as\n",
        "\n",
        "      œÉ(z) = 1/(1+e‚Åª·∂ª),\n",
        "maps any real number z into the interval (0, 1). This makes it ideal for representing probabilities in binary classification scenarios.\n",
        "\n",
        "* Interpretability through Log-Odds: The logistic model can be expressed in terms of log-odds (logit) as:\n",
        "\n",
        "      log(p/(1-p)) = z,\n",
        "where 'p' is the probability of the positive class. The sigmoid function is the inverse of this logit transformation, providing a direct link between a linear predictor and probability.\n",
        "\n",
        "* Differentiability:\n",
        "The sigmoid function is smooth and differentiable, which is a crucial property for optimization algorithms used in training logistic regression models. The derivative of the sigmoid function is particularly simple and computationally efficient, which helps in updating the model parameters during training.\n",
        "\n",
        "* Non-linearity:\n",
        "Although the model is linear in the parameters Œ≤, the sigmoid function introduces a non-linear transformation. This non-linearity is what allows logistic regression to model probabilities in a manner that reflects the underlying distribution of the data, even when the decision boundary is more complex."
      ],
      "metadata": {
        "id": "yhdz8PXsC5Vl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 4. What is the cost function of Logistic Regression?\n",
        "**Ans** - The cost function in logistic regression is typically defined using the log loss, which measures the error between the predicted probabilities and the actual binary outcomes. For a single training example with features x, actual label y, and predicted probability ≈∑ = œÉ(z) (with z = Œ≤‚ÇÄ+Œ≤‚ÇÅx‚ÇÅ+‚ãØ+Œ≤‚Çñx‚Çñ), the cost is given by:\n",
        "\n",
        "    cost(≈∑,y)=‚àí[ylog(≈∑)+(1‚àíy)log‚Å°(1‚àí≈∑)]\n",
        "For the entire training set of m examples, the cost function J(Œ≤) becomes the average of these individual costs:\n",
        "\n",
        "    J(Œ≤) = ‚àí1/m‚àë·µê·µ¢‚Çå‚ÇÅ[y‚ÅΩ·∂¶‚Åælog(≈∑‚ÅΩ·∂¶‚Åæ)+(1‚àíy‚ÅΩ·∂¶‚Åæ)log(1‚àí≈∑‚ÅΩ·∂¶‚Åæ)]\n",
        "This function is convex, which means it has a single global minimum. That property makes it well-suited for optimization techniques like gradient descent.\n",
        "\n",
        "**Key Points:**\n",
        "* Log Loss: The cost function is also known as log loss or binary cross-entropy.\n",
        "* Probability Interpretation: It directly relates to the probability estimates output by the sigmoid function.\n",
        "* Convexity: The convex nature of this cost function ensures that optimization algorithms converge to a global minimum."
      ],
      "metadata": {
        "id": "CmwxHsHUC7Ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 5. What is Regularization in Logistic Regression? Why is it needed?\n",
        "**Ans** - **Regularization in Logistic Regression**\n",
        "\n",
        "Regularization in Logistic Regression is a technique used to prevent overfitting by adding a penalty term to the loss function. It discourages the model from assigning excessively high weights to certain features, ensuring that the model generalizes well to unseen data.\n",
        "\n",
        "**Regularization is Needed**\n",
        "1. Prevents Overfitting: Without regularization, the model may memorize the training data instead of learning general patterns, leading to poor performance on new data.\n",
        "2. Controls Model Complexity: Large coefficient values indicate a complex model, which may not generalize well. Regularization reduces this risk by shrinking coefficients.\n",
        "3. Handles Multicollinearity: In datasets with highly correlated features, regularization helps stabilize the model by reducing the effect of redundant features.\n",
        "4. Improves Generalization: Ensures the model performs well not just on training data but also on unseen test data."
      ],
      "metadata": {
        "id": "Wwr0O-nnC8ht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "**Ans** - **Difference Between Lasso, Ridge, and Elastic Net Regression**\n",
        "\n",
        "Regularization in regression helps prevent overfitting by adding a penalty term to the loss function, which controls the size of the model's coefficients. The three main types are:\n",
        "\n",
        "|Type\t|Penalty Term\t|Effect on Coefficients\t|Use Case|\n",
        "|-||||\n",
        "|Lasso\t|( \\lambda \\sum\t|w_i\t|)|\n",
        "|Ridge |Œª‚àëw·µ¢¬≤\t|Shrinks coefficients closer to zero, but never exactly zero\t|When you want to keep all features, but reduce their impact.|\n",
        "|Elastic Net\t|(\\lambda (\\alpha \\sum\t|w_i\t|+ (1-\\alpha)\\sum w_i^2))\n",
        "\n",
        "**Explanation**\n",
        "\n",
        "**1. Lasso Regression**\n",
        "* Adds absolute values of coefficients |w·µ¢| to the cost function.\n",
        "* Encourages sparsity: Some coefficients become exactly zero, effectively selecting only important features.\n",
        "* Can be used for automatic feature selection.\n",
        "* Useful when the dataset has many irrelevant or redundant features.\n",
        "* Best for: Feature selection and reducing dimensionality.\n",
        "\n",
        "**2. Ridge Regression**\n",
        "* Adds the squared values of coefficients w·µ¢¬≤\n",
        "  to the cost function.\n",
        "* Shrinks coefficients but does not make them zero.\n",
        "* Helps in handling multicollinearity.\n",
        "* Keeps all features, just reduces their impact.\n",
        "* Best for: Preventing overfitting while keeping all features.\n",
        "\n",
        "**3. Elastic Net Regression**\n",
        "* Combines Lasso and Ridge:\n",
        "  * L1 helps in feature selection.\n",
        "  * L2 helps in stabilizing the model.\n",
        "* Useful when features are correlated, where Lasso alone may struggle.\n",
        "* Best for: Handling correlated features and balancing feature selection with model stability.\n",
        "\n",
        "**Choosing Between Lasso, Ridge, and Elastic Net**\n",
        "\n",
        "|Scenario\t|Best Regularization|\n",
        "|-||\n",
        "|Many irrelevant features?\t|Lasso|\n",
        "|Multicollinearity (correlated features)?\t|Ridge|\n",
        "|Need both feature selection & multicollinearity handling?\t|Elastic Net|"
      ],
      "metadata": {
        "id": "tSOcb9JoC-Dt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "**Ans** - **Use of Elastic Net Instead Lasso or Ridge**\n",
        "\n",
        "Elastic Net is preferred when you have high-dimensional data and highly correlated features, as it combines the strengths of both Lasso and Ridge regularization. Below are specific scenarios where Elastic Net is the best choice:\n",
        "\n",
        "**1. Features Are Highly Correlated**\n",
        "* Ridge Regression works well with correlated features but keeps all features in the model, just shrinking their coefficients.\n",
        "* Lasso Regression can randomly pick one correlated feature and force others to zero, which may lead to an unstable model.\n",
        "* Elastic Net combines both approaches, ensuring correlated features are selected together rather than arbitrarily dropping some.\n",
        "\n",
        "**2. We Need Feature Selection But Lasso Performs Poorly**\n",
        "* Lasso can eliminate many features by forcing coefficients to zero, but in some cases (eg.- when there are more predictors than observations), it might select too few features or be unstable.\n",
        "* Elastic Net avoids this issue by maintaining Ridge's ability to spread influence across correlated variables while still enforcing sparsity.\n",
        "\n",
        "**3. We have More Features Than Observations**\n",
        "* When the number of features (p) is much larger than the number of observations (n), such as in genetics or text processing, Lasso may struggle because it only selects a few predictors.\n",
        "* Ridge alone won't perform feature selection, so it may not be ideal either.\n",
        "* Elastic Net balances these by selecting relevant features while reducing coefficient magnitude for better stability.\n",
        "\n",
        "**4. We Want a Mix of Feature Selection and Coefficient Shrinking**\n",
        "* Lasso forces some coefficients to exactly zero, completely removing some features.\n",
        "* Ridge shrinks coefficients but never makes them exactly zero.\n",
        "* Elastic Net lets you control how much selection vs. shrinkage you want using the mixing parameter Œ±.\n",
        "  * Œ± = 1 ‚Üí Pure Lasso\n",
        "  * Œ± = 0 ‚Üí Pure Ridge\n",
        "  * 0 < Œ± < 1 ‚Üí Hybrid effect"
      ],
      "metadata": {
        "id": "yg4IB_FlC_Ok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 8. What is the impact of the regularization parameter (Œª) in Logistic Regression?\n",
        "**Ans** - **Impact of the Regularization Parameter (Œª) in Logistic Regression**\n",
        "The regularization parameter Œª in logistic regression controls the strength of regularization. It determines how much the model penalizes large coefficients, impacting model complexity, performance, and generalization.\n",
        "\n",
        "**Effect of Œª on Model Performance**\n",
        "* When Œª is too small\n",
        "  * Minimal regularization.\n",
        "  * Model relies heavily on the training data, potentially leading to overfitting.\n",
        "  * Coefficients remain large, capturing noise in the data.\n",
        "  * High variance, poor generalization to new data.\n",
        "* When Œª is too large\n",
        "  * Strong regularization; coefficients shrink towards zero.\n",
        "  * The model becomes simpler but may underfit the data.\n",
        "  * Important features might be ignored, reducing accuracy.\n",
        "  * High bias, good generalization but possibly poor predictive power.\n",
        "* When Œª is optimal\n",
        "  * A good balance between bias and variance.\n",
        "  * The model captures meaningful patterns while ignoring noise.\n",
        "  * Features with minor contributions may be reduced, improving interpretability.\n",
        "\n",
        "**Œª Affects L1, L2, and Elastic Net**\n",
        "* Ridge Regression:\n",
        "  * Larger Œª shrinks coefficients but does not make them exactly zero.\n",
        "  * Good for handling multicollinearity.\n",
        "* Lasso Regression:\n",
        "  * Larger Œª forces some coefficients to exactly zero, performing feature selection.\n",
        "  * Useful when only a subset of features is important.\n",
        "* Elastic Net:\n",
        "  * Balances feature selection and shrinkage.\n",
        "  * Adjusts Œª and mixing parameter Œ± to tune the balance.\n",
        "\n",
        "**Choosing the Right Œª**\n",
        "* Use cross-validation to find the optimal Œª.\n",
        "* Typically, higher-dimensional data requires more regularization.\n",
        "* If your model is overfitting, increase Œª; if it's underfitting, decrease Œª."
      ],
      "metadata": {
        "id": "hIXxyofIDAbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 9. What are the key assumptions of Logistic Regression?\n",
        "**Ans** - **Assumptions of Logistic Regression**\n",
        "Logistic regression is a robust and widely used classification algorithm, it relies on certain assumptions for optimal performance.\n",
        "\n",
        "1. The Dependent Variable is Binary\n",
        "  * Logistic regression assumes that the target variable y has only two categories (e.g., yes/no, spam/not spam, 0/1).\n",
        "  * For multiclass classification, a variation called Multinomial Logistic Regression is used.\n",
        "\n",
        "2. Independent Variables are Linearly Related to the Log-Odds\n",
        "  * Logistic regression does not assume a linear relationship between the independent variables (x‚ÇÅ,x‚ÇÇ,‚Ä¶,x‚Çñ) and the dependent variable y.\n",
        "  * However, it assumes that the log-odds of the dependent variable is linearly related to the independent variables:\n",
        "\n",
        "          log(p/(1-p)) = Œ≤‚ÇÄ+Œ≤‚ÇÅx‚ÇÅ+Œ≤‚ÇÇx‚ÇÇ+‚ãØ+Œ≤‚Çñx‚Çñ\n",
        "\n",
        "3. No Perfect Multicollinearity Among Independent Variables\n",
        "  * Logistic regression assumes that the independent variables are not highly correlated with each other.\n",
        "  * Multicollinearity can cause instability in coefficient estimates.\n",
        "\n",
        "4. No Auto-Correlation\n",
        "  * If the data is sequential, logistic regression assumes that observations are independent of each other.\n",
        "  * Violations of this assumption lead to biased estimates.\n",
        "\n",
        "5. Large Sample Size\n",
        "  * Logistic regression performs better with a large number of observations, especially for rare events.\n",
        "  * If the dataset is too small, the model may not accurately estimate coefficients.\n",
        "\n",
        "6. No Extreme Outliers\n",
        "  * Logistic regression is sensitive to outliers in the independent variables, as they can distort coefficients and influence predictions.\n",
        "\n",
        "7. Independent Variables Should Not Contain Missing Data\n",
        "  * Logistic regression does not handle missing values automatically.\n",
        "  * If missing data is present, it can lead to biased estimates.\n",
        "\n",
        "**Summary**\n",
        "\n",
        "|Assumption\t|Solution if Violated|\n",
        "|-||\n",
        "|Binary dependent variable\t|Use multinomial or ordinal logistic regression if needed.|\n",
        "|Log-odds are linearly related to predictors\t|Use polynomial/log-transformed variables if the relationship is nonlinear.|\n",
        "|No multicollinearity\t|Check VIF, remove correlated variables, or use Ridge regression.|\n",
        "|No autocorrelation (for time series)\t|Use time-series models or introduce lag variables.|\n",
        "|Large sample size\t|Use bootstrapping or Bayesian approaches for small datasets.|\n",
        "|No extreme outliers\t|Use robust scaling, transformations, or regularization.|\n",
        "|No missing data in predictors\t|Use imputation methods (mean, median, KNN).|"
      ],
      "metadata": {
        "id": "Kkprx4yvDBtF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "**Ans** - **Alternatives to Logistic Regression for Classification Tasks**\n",
        "\n",
        "While logistic regression is a simple and effective classification model, it may not always perform well, especially with complex relationships, high-dimensional data, or nonlinear decision boundaries.\n",
        "\n",
        "1. Decision Trees\n",
        "  * Splits data into branches based on feature thresholds.\n",
        "  * Creates a tree-like structure where each leaf represents a class label.\n",
        "* Pros:\n",
        "  * Handles nonlinear relationships and feature interactions well.\n",
        "  * Works with both numerical and categorical data.\n",
        "  * No need for feature scaling.\n",
        "\n",
        "2. Random Forest\n",
        "  * An ensemble of multiple decision trees trained on different subsets of data.\n",
        "  * Final prediction is based on majority voting or averaging.\n",
        "* Pros:\n",
        "  * Reduces overfitting.\n",
        "  * Handles missing values and high-dimensional data well.\n",
        "  * Works for both binary and multiclass classification.\n",
        "\n",
        "3. Support Vector Machines\n",
        "  * Finds the optimal hyperplane that maximizes the margin between classes.\n",
        "  * Can use kernel tricks to handle nonlinear relationships.\n",
        "* Pros:\n",
        "  * Effective in high-dimensional spaces.\n",
        "  * Works well with small datasets.\n",
        "  * Can handle nonlinear classification using kernel functions.\n",
        "\n",
        "4. k-Nearest Neighbors\n",
        "  * Assigns a class label based on the majority vote of the k closest data points.\n",
        "* Pros:\n",
        "  * Simple and intuitive.\n",
        "  * No need for model training‚Äîjust store the data.\n",
        "\n",
        "5. Na√Øve Bayes\n",
        "  * Uses Bayes' Theorem to compute the probability of each class given the input features.\n",
        "  * Assumes independence between features.\n",
        "* Pros:\n",
        "  * Fast and efficient, even for large datasets.\n",
        "  * Works well with text classification.\n",
        "\n",
        "6. Artificial Neural Networks\n",
        "  * Uses multiple layers of neurons to extract patterns from data.\n",
        "  * Can learn complex, nonlinear relationships.\n",
        "* Pros:\n",
        "  * Highly flexible, can model very complex patterns.\n",
        "  * Best suited for image, speech, and natural language processing tasks.\n",
        "\n",
        "7. Gradient Boosting Machines\n",
        "  * Uses boosting to combine weak classifiers into a strong model.\n",
        "* Pros:\n",
        "  * Highly accurate, often used in Kaggle competitions.\n",
        "  * Handles missing values and imbalanced datasets well.\n",
        "\n",
        "**Comparison Table**\n",
        "\n",
        "|Model\t|Handles Nonlinearity?\t|Works with Small Data?\t|Works with Large Data?\t|Handles High-Dimensional Data?\t|Feature Importance|\n",
        "|-||||||\n",
        "|Logistic Regression\t|No\t|Yes\t|Yes\t|No\t|Yes|\n",
        "|Decision Tree\t|Yes\t|Yes\t|Yes\t|No\t|Yes|\n",
        "|Random Forest\t|Yes\t|Yes\t|Yes\t|Yes\t|Yes|\n",
        "|SVM\t|Yes (with kernel)\t|Yes\t|No\t|Yes\t|No|\n",
        "|k-NN\t|Yes\t|Yes\t|No\t|No\t|No|\n",
        "|Na√Øve Bayes\t|No (assumes independence)\t|Yes\t|Yes\t|No\t|No|\n",
        "|Neural Networks\t|Yes\t|No\t|Yes\t|Yes\t|No|\n",
        "|Gradient Boosting (XGBoost, LightGBM)\t|Yes\t|Yes\t|Yes\t|Yes\t|Yes|\n",
        "\n",
        "**Model Should we Choose**\n",
        "* If data is small and linear ‚Üí Logistic Regression or SVM\n",
        "* If data is nonlinear ‚Üí Decision Trees, Random Forest, or SVM\n",
        "* If interpretability is important ‚Üí Logistic Regression, Decision Trees\n",
        "* If accuracy is the main goal ‚Üí Gradient Boosting, Neural Networks\n",
        "* If data is high-dimensional ‚Üí SVM, Neural Networks, Gradient Boosting"
      ],
      "metadata": {
        "id": "Tgg9Y3d9DCxe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 11. What are Classification Evaluation Metrics?\n",
        "**Ans** - **Classification Evaluation Metrics**\n",
        "\n",
        "Evaluating a classification model is crucial to understanding its performance. Most commonly used classification evaluation metrics are:\n",
        "\n",
        "1. Accuracy\n",
        "* Accuracy measures the percentage of correctly classified instances out of the total instances.\n",
        "\n",
        "      Accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
        "* Pros:\n",
        "  * Simple and easy to interpret.\n",
        "  * Works well when classes are balanced.\n",
        "\n",
        "2. Precision\n",
        "* Precision measures the proportion of correctly predicted positive instances among all predicted positives.\n",
        "\n",
        "      Precision = TP/(TP + FP)\n",
        "* Pros:\n",
        "  * Useful when false positives are costly eg.- spam detection, medical diagnosis.\n",
        "\n",
        "3. Recall\n",
        "  * Recall measures the proportion of actual positive instances that were correctly identified.\n",
        "\n",
        "      Recall = TP/(TP + FN)\n",
        "* Pros:\n",
        "  * Useful when false negatives should be minimized eg.- medical diagnosis, security applications.\n",
        "\n",
        "4. F1 Score\n",
        "  * F1 Score provides a balance between precision and recall, especially when data is imbalanced.\n",
        "\n",
        "      F1 Score = 2 * (Precision x Recall)/(Precision + Recall)\n",
        "* Pros:\n",
        "  * Good balance between precision and recall.\n",
        "  * Works well for imbalanced datasets.\n",
        "\n",
        "5. Specificity\n",
        "  * Measures the proportion of actual negative instances correctly classified.\n",
        "\n",
        "      Specificity = TN/(TN + FP)\n",
        "* Pros:\n",
        "  * Useful when detecting negative instances is important eg.- screening non-diseased patients.\n",
        "\n",
        "6. ROC Curve & AUC\n",
        "  * The ROC curve plots True Positive Rate vs. False Positive Rate at different classification thresholds.\n",
        "  * AUC measures the ability of a model to separate positive and negative classes.\n",
        "* Pros:\n",
        "  * Useful for evaluating models across different probability thresholds.\n",
        "\n",
        "7. PR Curve\n",
        "  * The Precision-Recall curve plots precision vs. recall at different classification thresholds.\n",
        "  * Useful when data is imbalanced.\n",
        "* Pros:\n",
        "  * Better than ROC for imbalanced datasets.\n",
        "\n",
        "**Comparison of Metrics & When to Use Them**\n",
        "\n",
        "|Metric\t|Best Use Case\t|Handles Imbalanced Data?|\n",
        "|-|||\n",
        "|Accuracy\t|Balanced datasets\t|No|\n",
        "|Precision\t|When false positives are costly\t|Yes|\n",
        "|Recall\t|When false negatives are costly\t|Yes|\n",
        "|F1 Score\t|When both FP & FN matter\t|Yes|\n",
        "|Specificity\t|When true negatives are important\t|Yes|\n",
        "|ROC-AUC\t|When evaluating model discrimination power\t|Yes|\n",
        "|PR Curve\t|When data is imbalanced\t|Yes|\n",
        "|Log Loss\t|When dealing with probability-based classification\t|Yes|"
      ],
      "metadata": {
        "id": "o9op5acMDELP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 12. How does class imbalance affect Logistic Regression?\n",
        "**Ans** - Class imbalance occurs when one class is significantly more frequent than the other in a dataset. This can cause logistic regression to become biased toward the majority class, leading to poor predictive performance for the minority class.\n",
        "\n",
        "1. Effects of Class Imbalance on Logistic Regression\n",
        "\n",
        "  **a. Biased Decision Boundary**\n",
        "* Logistic regression learns a decision boundary by minimizing the overall error.\n",
        "* If one class dominates, the model prioritizes correctly classifying the majority class over the minority class.\n",
        "* As a result, the decision boundary is skewed toward the majority class, making it harder to correctly classify the minority class.\n",
        "\n",
        "  **b. Misleading Accuracy**\n",
        "* In highly imbalanced datasets, accuracy becomes misleading because the model can achieve high accuracy just by predicting the majority class.\n",
        "  * Example:\n",
        "    * 90% majority class, 10% minority class\n",
        "    * If the model always predicts the majority class, accuracy = 90%, but it completely fails to predict the minority class.\n",
        "\n",
        "  **c. Poor Precision, Recall, and F1 Score**\n",
        "* Low recall for the minority class ‚Üí Many false negatives.\n",
        "* Low precision if the model incorrectly classifies majority instances as minority.\n",
        "* Low F1-score because the imbalance distorts both precision and recall.\n",
        "\n",
        "  **d. Poor Calibration of Predicted Probabilities**\n",
        "* Logistic regression outputs probabilities using the sigmoid function, but in an imbalanced dataset, these probabilities become skewed.\n",
        "* The model assigns low probability scores to minority class instances, making it difficult to set a threshold that effectively separates the two classes.\n",
        "\n",
        "2. Handle Class Imbalance in Logistic Regression\n",
        "\n",
        "Several techniques can help logistic regression perform better in imbalanced datasets:\n",
        "\n",
        "**a. Resampling Techniques**\n",
        "* Oversampling the Minority Class (SMOTE - Synthetic Minority Over-sampling Technique)\n",
        "  * Generates synthetic examples to balance class distribution.\n",
        "  * Helps the model learn more about the minority class.\n",
        "\n",
        "* Undersampling the Majority Class\n",
        "  * Removes some majority class samples to create a balanced dataset.\n",
        "  * Useful when data is large, but it may lose important information.\n",
        "* Combination of Over- and Under-sampling\n",
        "  * Uses both SMOTE and undersampling to maintain data diversity.\n",
        "\n",
        "**b. Adjusting Class Weights**\n",
        "* Use class_weight='balanced' in Sklearn's Logistic Regression\n",
        "* Assigns higher weights to the minority class and lower weights to the majority class.\n",
        "* Helps in reducing the bias toward the majority class."
      ],
      "metadata": {
        "id": "SgRfTjWUDGM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(class_weight='balanced')"
      ],
      "metadata": {
        "id": "5D4960SceFbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Manually Setting Class Weights\n",
        "  * If class imbalance is extreme (eg.- 95% majority, 5% minority), manually set the weights:"
      ],
      "metadata": {
        "id": "_QF04c-geJaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(class_weight={0:1, 1:10})"
      ],
      "metadata": {
        "id": "48wGrGxxeSxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c. Changing the Decision Threshold**\n",
        "* By default, logistic regression assigns class labels using 0.5 as the probability threshold.\n",
        "* In imbalanced data, a lower threshold can improve recall for the minority class.\n",
        "\n",
        "**Example**"
      ],
      "metadata": {
        "id": "lB1Y7yrOeVqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
        "optimal_threshold = thresholds[np.argmax(precision * recall)]"
      ],
      "metadata": {
        "id": "4PdqAkKdeen9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**d. Using Evaluation Metrics Suitable for Imbalanced Data**\n",
        "* Precision, Recall, F1-score instead of accuracy.\n",
        "* ROC-AUC or PR Curve to compare models.\n",
        "* Confusion Matrix to analyze true positives/negatives and false positives/negatives.\n",
        "\n",
        "**Example**"
      ],
      "metadata": {
        "id": "3l3Z_M4Ueh_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "I60CZHzhesWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Summary**\n",
        "\n",
        "|Issue\t|Impact on Logistic Regression\t|Solution|\n",
        "|-|||\n",
        "|Biased Decision Boundary\t|Skewed toward majority class\t|Resampling, class weighting|\n",
        "|Misleading Accuracy\t|High accuracy but poor recall\t|Use F1-score, ROC-AUC|\n",
        "|Low Recall for Minority Class\t|Model misses minority class\t|Lower threshold, oversampling|\n",
        "|Probability Calibration Issues\t|Wrong confidence in predictions\t|Adjust decision threshold|"
      ],
      "metadata": {
        "id": "pxEGaYVQevDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "**Ans** - **Hyperparameter Tuning in Logistic Regression**\n",
        "\n",
        "Hyperparameter tuning is the process of optimizing the parameters that control how a machine learning model learns. In logistic regression, hyperparameters are not learned from data but set manually before training. Choosing the right hyperparameters can improve accuracy, generalization, and robustness of the model.\n",
        "\n",
        "**1. Hyperparameters in Logistic Regression**\n",
        "\n",
        "a. Regularization Parameter (Œª or C)\n",
        "* Controls the strength of L1 or L2 regularization.\n",
        "* Helps prevent overfitting or underfitting.\n",
        "* C (inverse of Œª in scikit-learn):\n",
        "  * Higher C ‚Üí More complex model, risk of overfitting.\n",
        "  * Lower C ‚Üí Simpler model, risk of underfitting.\n",
        "\n",
        "Tuning Strategy:\n",
        "* Try different values of C (eg.- 10^-4 to 10^4) using Grid Search or Random Search.\n",
        "\n",
        "b. Type of Regularization\n",
        "* L1 ‚Üí Shrinks some coefficients to zero.\n",
        "* L2 ‚Üí Shrinks all coefficients equally, reducing overfitting.\n",
        "* Elastic Net ‚Üí Mix of L1 and L2 regularization.\n",
        "\n",
        "Tuning Strategy:\n",
        "* Choose L1 if feature selection is needed.\n",
        "* Choose L2 for better generalization.\n",
        "* Choose Elastic Net for a balance.\n",
        "\n",
        "c. Solver\n",
        "\n",
        "Different optimization algorithms for logistic regression:\n",
        "* liblinear ‚Üí Works well for small datasets and L1 regularization.\n",
        "* lbfgs ‚Üí Default, works for large datasets and L2 regularization.\n",
        "* saga ‚Üí Best for large datasets with L1, L2, or Elastic Net.\n",
        "* newton-cg ‚Üí Works for L2 regularization but not L1.\n",
        "\n",
        "Tuning Strategy:\n",
        "* Try liblinear for small datasets, saga for large datasets.\n",
        "\n",
        "d. Class Weight (class_weight)\n",
        "\n",
        "Used to handle class imbalance:\n",
        "* \"balanced\" ‚Üí Automatically adjusts weights inversely proportional to class frequencies.\n",
        "* Custom values {0:1, 1:5} ‚Üí Higher weight for the minority class.\n",
        "\n",
        "Tuning Strategy:\n",
        "* Use \"balanced\" if dataset is highly imbalanced.\n",
        "\n",
        "**2. Hyperparameter Tuning Methods**\n",
        "\n",
        "a. Grid Search\n",
        "* Tries all possible combinations of hyperparameters.\n",
        "\n",
        "**Example**"
      ],
      "metadata": {
        "id": "PWNEl0a-DHd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "log_reg = LogisticRegression()\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='f1')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "g7g6D38igSKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Random Search (Faster than Grid Search)\n",
        "\n",
        "* Samples random combinations instead of testing all\n",
        "\n",
        "**Example**"
      ],
      "metadata": {
        "id": "L54Hx2ShgVjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform\n",
        "\n",
        "param_dist = {'C': uniform(0.001, 10)}\n",
        "\n",
        "random_search = RandomizedSearchCV(LogisticRegression(), param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy')\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(random_search.best_params_)"
      ],
      "metadata": {
        "id": "CXtKfDjsgfJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. Bayesian Optimization & Genetic Algorithms\n",
        "* Advanced techniques for large datasets.\n",
        "* Libraries: Optuna, Hyperopt.\n",
        "\n",
        "**3. Summary**\n",
        "\n",
        "|Hyperparameter\t|Purpose\t|Tuning Strategy|\n",
        "|-|||\n",
        "|C (Regularization strength)\t|Prevents overfitting/underfitting\t|Try different values (e.g., 0.001 to 100)|\n",
        "|Penalty (L1/L2/Elastic Net)\t|Feature selection, regularization\t|L1 for feature selection, L2 for generalization|\n",
        "|Solver\t|Optimization method\t|\"liblinear\" for small datasets, \"saga\" for large|\n",
        "|Class Weight\t|Handles imbalance\t|\"balanced\" or custom values {0:1, 1:5}|"
      ],
      "metadata": {
        "id": "hPB7eR9KgisF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 14. What are different solvers in Logistic Regression? Which one should be used?\n",
        "**Ans** - **Solvers in Logistic Regression & When to Use Them**\n",
        "\n",
        "A solver is an optimization algorithm used to minimize the cost function in logistic regression. Different solvers work better depending on dataset size, regularization type, and computational efficiency.\n",
        "\n",
        "**1. Types of Solvers in Scikit-Learn's Logistic Regression**\n",
        "\n",
        "|Solver\t|Type of Optimization\t|Supports L1 Regularization?\t|Supports L2 Regularization?\t|Best for|\n",
        "|-|||||\n",
        "|liblinear\t|Coordinate Descent\t|Yes\t|Yes\t|Small datasets, L1 regularization|\n",
        "|lbfgs\t|Quasi-Newton Method\t|No\t|Yes\t|Large datasets, L2 regularization|\n",
        "|newton-cg\t|Newton's Method\t|No\t|Yes\t|Large datasets, L2 regularization|\n",
        "|saga\t|Stochastic Gradient Descent\t|Yes\t|Yes\t|Large datasets, L1/L2/Elastic Net|\n",
        "|sag\t|Stochastic Average Gradient\t|No\t|Yes\t|Large datasets with many features|\n",
        "\n",
        "**2. Choosing the Right Solver**\n",
        "\n",
        "* For Small Datasets (< 10,000 samples)\n",
        "  * Use \"liblinear\" (Supports L1 & L2, works well with few data points).\n",
        "\n",
        "* For Large Datasets (>10,000 samples)\n",
        "  * Use \"saga\" if you need L1, L2, or Elastic Net regularization.\n",
        "  * Use \"lbfgs\" or \"newton-cg\" for L2 regularization.\n",
        "\n",
        "* For Highly Imbalanced Datasets\n",
        "  * Use \"saga\" or \"sag\" because they handle large data efficiently and can be combined with class weighting.\n",
        "\n",
        "**3. When to Avoid Certain Solvers**\n",
        "\n",
        "|Solver\t|Avoid When|\n",
        "|-||\n",
        "|liblinear\t|Large datasets|\n",
        "|lbfgs\t|Need L1 regularization|\n",
        "|newton-cg\t|Need L1 regularization|\n",
        "|sag\t|Small datasets|\n",
        "\n",
        "**4. Example: Choosing the Solver in Scikit-Learn**"
      ],
      "metadata": {
        "id": "ABi31hy8DI02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(solver='liblinear', penalty='l1')\n",
        "\n",
        "model = LogisticRegression(solver='lbfgs', penalty='l2')\n",
        "\n",
        "model = LogisticRegression(solver='saga', penalty='l1')"
      ],
      "metadata": {
        "id": "qB6HPxXoiwsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Summary**\n",
        "\n",
        "|Scenario\t|Best Solver|\n",
        "|-||\n",
        "|Small dataset\t|liblinear|\n",
        "|Large dataset\t|saga or lbfgs|\n",
        "|L1 Regularization\t|liblinear or saga|\n",
        "|L2 Regularization\t|lbfgs, sag, or newton-cg|\n",
        "|Elastic Net Regularization\t|saga|\n",
        "|Imbalanced Dataset\t|saga or sag|"
      ],
      "metadata": {
        "id": "rNZjyVDOi1a_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 15. How is Logistic Regression extended for multiclass classification?\n",
        "**Ans** - **Extending Logistic Regression for Multiclass Classification**\n",
        "\n",
        "Logistic regression is naturally designed for binary classification. To handle multiclass classification, it is extended using strategies like One-vs-Rest and One-vs-One. Another approach is the softmax function.\n",
        "\n",
        "**1. Approaches for Multiclass Logistic Regression**\n",
        "\n",
        "üîπ 1. One-vs-Rest\n",
        "* Concept:\n",
        "  * Train one binary classifier per class.\n",
        "  * Each classifier distinguishes one class from the rest.\n",
        "  * The class with the highest probability is chosen.\n",
        "* Example (Classes: A, B, C)\n",
        "  * Model 1: A vs (B, C)\n",
        "  * Model 2: B vs (A, C)\n",
        "  * Model 3: C vs (A, B)\n",
        "  * Predict argmax.\n",
        "\n",
        "* Pros:\n",
        "  * Simple and efficient for many classes.\n",
        "  * Works with any binary classifier.\n",
        "\n",
        "* Scikit-Learn Implementation"
      ],
      "metadata": {
        "id": "5RrqPYgIDK23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "vHSDXyS-kEzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. One-vs-One**\n",
        "* Concept:\n",
        "  * Train one binary classifier per class pair.\n",
        "  * A total of N(N-1)/2 classifiers.\n",
        "  * During prediction, each classifier votes, and the class with the most votes is chosen.\n",
        "* Example (Classes: A, B, C)\n",
        "  * Model 1: A vs B\n",
        "  * Model 2: A vs C\n",
        "  * Model 3: B vs C\n",
        "  * Predict class with majority votes.\n",
        "* Pros:\n",
        "  * Works well when classes are similar.\n",
        "  * Computationally efficient for small N.\n",
        "\n",
        "* Scikit-Learn Implementation"
      ],
      "metadata": {
        "id": "YIhbeS1akLE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = OneVsOneClassifier(LogisticRegression(solver='liblinear'))\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "G1EXd9CAllls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Softmax Regression**\n",
        "* Concept:\n",
        "  * Extends logistic regression to directly predict multiple classes.\n",
        "  * Uses the softmax function to convert scores into probabilities.\n",
        "  * Chooses the class with the highest softmax probability.\n",
        "\n",
        "* Pros:\n",
        "  * More efficient than OvR/OvO.\n",
        "  * Better calibrated probabilities.\n",
        "\n",
        "* Scikit-Learn Implementation"
      ],
      "metadata": {
        "id": "M6nCTcqFlpVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "vWpgIDP5l21M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Comparison of Methods**\n",
        "\n",
        "|Method\t|Number of Models\t|Prediction Speed\t|Best When...|\n",
        "|-||||\n",
        "|One-vs-Rest\t|N\t|Fast\t|Simple, small datasets|\n",
        "|One-vs-One\t|N(N-1)/2\t|Slow\t|Small N, but better classification|\n",
        "|Softmax Regression\t|1\t|Fast\t|Large datasets, better probability estimation|\n",
        "\n",
        "**3. Method should we use**\n",
        "* Use multinomial if your dataset is large and well-structured.\n",
        "* Use ovr if you prefer binary classifiers and want simplicity.\n",
        "* Use ovo only if you have few classes and need pairwise comparisons."
      ],
      "metadata": {
        "id": "IEuZn455l56U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 16. What are the advantages and disadvantages of Logistic Regression?\n",
        "**Ans** - **Advantages of Logistic Regression**\n",
        "1. Simple and Interpretable\n",
        "  * Easy to understand compared to complex models like neural networks.\n",
        "  * The coefficients provide insights into feature importance.\n",
        "2. Efficient for Binary Classification\n",
        "  * Works well when the target variable has two classes (eg.- spam vs. not spam).\n",
        "3. Less Prone to Overfitting\n",
        "  * L1 and L2 regularization help prevent overfitting.\n",
        "4. Computationally Efficient\n",
        "  * Faster training and inference compared to deep learning or ensemble methods.\n",
        "5. Works Well with Small & Medium Datasets\n",
        "  * Performs well even with a limited amount of data.\n",
        "6. Can Handle Linearly Separable Data Well\n",
        "  * If the classes can be separated by a straight line, logistic regression performs very well.\n",
        "7. Can Output Probabilities\n",
        "  * Unlike decision trees, logistic regression provides probability estimates for each class.\n",
        "\n",
        "**Disadvantages of Logistic Regression**\n",
        "1. Assumes Linearity Between Features and Log-Odds\n",
        "  * Doesn't perform well when the decision boundary is non-linear.\n",
        "  * Fails when there are complex relationships between features.\n",
        "2. Not Ideal for High-Dimensional Data\n",
        "  * Struggles when there are too many features relative to the number of samples.\n",
        "  * Overfitting can occur without proper regularization.\n",
        "3. Sensitive to Outliers\n",
        "  * Outliers can significantly impact the model's coefficients.\n",
        "  * Using robust scaling or removing outliers can help.\n",
        "4. Struggles with Multiclass Classification\n",
        "  * Requires extensions like One-vs-Rest or Softmax Regression for multiple classes.\n",
        "5. Requires Feature Engineering\n",
        "  * Logistic regression does not automatically handle feature interactions.\n",
        "  * Needs careful feature selection and transformation (eg.- polynomial features).\n",
        "6. Can't Handle Highly Correlated Features\n",
        "  * If two or more features are highly correlated, it can affect coefficient estimation.\n",
        "  * Solution: Use Principal Component Analysis or remove redundant features.\n",
        "7. Assumes No Missing Data\n",
        "  * Logistic regression cannot handle missing values natively.\n",
        "  * Requires imputation or dropping missing data before training."
      ],
      "metadata": {
        "id": "XDmzIhAuDN61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 17. What are some use cases of Logistic Regression?\n",
        "**Ans** -  **Use Cases of Logistic Regression**\n",
        "\n",
        "Logistic Regression is widely used for binary and multiclass classification problems across different domains. Here are some key real-world applications:\n",
        "\n",
        "**1. Healthcare & Medical Diagnosis**\n",
        "* Disease Prediction & Diagnosis\n",
        "  * Example: Predicting whether a patient has diabetes, heart disease, or cancer based on symptoms and test results.\n",
        "  * Logistic regression is interpretable and helps doctors understand how each feature (eg.- blood pressure, glucose levels) contributes to risk.\n",
        "\n",
        "* Example Dataset:\n",
        "  * Inputs: Age, BMI, blood pressure, glucose level.\n",
        "  * Output: 1 (Diabetic) / 0 (Non-Diabetic)."
      ],
      "metadata": {
        "id": "im00g0ibDPt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "ZpqQiuEYbyZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Banking & Finance**\n",
        "* Credit Scoring & Loan Approval\n",
        "  * Example: Predicting whether a customer will default on a loan based on financial history.\n",
        "  * Helps banks assess risk and make lending decisions.\n",
        "\n",
        "* Example Dataset:\n",
        "  * Inputs: Credit history, income, debt-to-income ratio.\n",
        "  * Output: 1 (Default) / 0 (No Default).\n",
        "\n",
        "* Fraud Detection\n",
        "  * Example: Detecting fraudulent credit card transactions.\n",
        "  * Identifies patterns in transaction behavior."
      ],
      "metadata": {
        "id": "qU-lqneJb2Rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "oTKtgrOWcNnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Marketing & Customer Analytics**\n",
        "* Customer Churn Prediction\n",
        "  * Example: Predicting whether a customer will stop using a service (eg.- canceling a subscription).\n",
        "  * Helps businesses retain customers by identifying high-risk churners.\n",
        "\n",
        "* Example Dataset:\n",
        "  * Inputs: Monthly usage, support calls, subscription plan.\n",
        "  * Output: 1 (Churn) / 0 (Retained).\n",
        "\n",
        "* Email Spam Detection\n",
        "  * Example: Classifying emails as spam or not spam based on text features.\n",
        "  * Fast, efficient, and interpretable classification."
      ],
      "metadata": {
        "id": "Q1yMuD8mcR-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "XUTy4xmzcwLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Human Resources**\n",
        "* Employee Attrition Prediction\n",
        "  * Example: Predicting if an employee will leave a company based on work hours, salary, and satisfaction.\n",
        "  * Helps HR teams improve retention strategies.\n",
        "* Example Dataset:\n",
        "  * Inputs: Salary, promotion history, work-life balance.\n",
        "  * Output: 1 (Leaves) / 0 (Stays).\n",
        "\n",
        "**5. Social Media & Sentiment Analysis**\n",
        "* Sentiment Analysis\n",
        "  * Example: Classifying positive vs. negative customer reviews or tweets.\n",
        "  * Helps brands understand customer opinions.\n",
        "* Example Dataset:\n",
        "  * Inputs: Words used in a tweet or review.\n",
        "  * Output: 1 (Positive) / 0 (Negative)."
      ],
      "metadata": {
        "id": "ViSjKjTFczgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "id": "u1CehPlbdJRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Fake News Detection\n",
        "  * Example: Detecting whether a news article is real or fake based on textual patterns.\n",
        "\n",
        "**6. Manufacturing & Quality Control**\n",
        "* Defect Detection in Production Lines\n",
        "  * Example: Predicting whether a manufactured product has defects based on sensor data.\n",
        "  * Helps improve quality control.\n",
        "* Example Dataset:\n",
        "  * Inputs: Temperature, pressure, machine vibrations.\n",
        "  * Output: 1 (Defective) / 0 (Not Defective).\n",
        "\n",
        "**7. Criminal Justice & Law Enforcement**\n",
        "* Crime Prediction & Recidivism Risk\n",
        "  * Example: Predicting whether a criminal is likely to reoffend after release.\n",
        "  * Helps make better parole decisions.\n",
        "* Example Dataset:\n",
        "  * Inputs: Criminal history, age, socioeconomic factors.\n",
        "  * Output: 1 (Reoffends) / 0 (Does Not Reoffend).\n",
        "\n",
        "**8. Autonomous Vehicles & Traffic Management**\n",
        "* Accident Prediction\n",
        "  * Example: Predicting whether certain road conditions or driver behaviors lead to accidents.\n",
        "* Traffic Sign Recognition\n",
        "  * Example: Classifying images of traffic signs for autonomous driving.\n",
        "\n",
        "**Logistic Regression Use Cases**\n",
        "\n",
        "|Industry\t|Use Case\t|Target Variable|\n",
        "|-|||\n",
        "|Healthcare\t|Disease diagnosis\t|1 (Disease) / 0 (No Disease)|\n",
        "|Finance\t|Loan default prediction\t|1 (Default) / 0 (No Default)|\n",
        "|Finance\t|Fraud detection\t|1 (Fraud) / 0 (No Fraud)|\n",
        "|Marketing\t|Customer churn prediction\t|1 (Churn) / 0 (Retained)|\n",
        "|Marketing\t|Email spam detection\t|1 (Spam) / 0 (Not Spam)|\n",
        "|HR\t|Employee attrition\t|1 (Leaves) / 0 (Stays)|\n",
        "|Social Media\t|Sentiment analysis\t|1 (Positive) / 0 (Negative)|\n",
        "|Law Enforcement\t|Crime prediction\t|1 (Reoffends) / 0 (No Reoffense)|\n",
        "|Manufacturing\t|Defect detection\t|1 (Defective) / 0 (Not Defective)|"
      ],
      "metadata": {
        "id": "-cQfpvnmdMTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "**Ans** - **Difference Between Softmax Regression and Logistic Regression**\n",
        "\n",
        "Logistic Regression and Softmax Regression are both classification techniques, but they differ in how they handle binary vs. multiclass classification.\n",
        "\n",
        "**1. Logistic Regression**\n",
        "* Used for:\n",
        "  * Binary classification, eg.- Spam (1) vs. Not Spam (0).\n",
        "* Working:\n",
        "  * Uses the Sigmoid function to output a probability between 0 and 1.\n",
        "  * If probability > threshold (eg.- 0.5) ‚Üí Class 1, else ‚Üí Class 0.\n",
        "* Sigmoid Function Formula:\n",
        "\n",
        "      P(y=1|X) = 1/(1+e‚Åª‚ÅΩ·µÇ·µÄÀ£‚Åæ)\n",
        "  * W = Weight vector\n",
        "  * X = Input features\n",
        "* Decision Boundary:\n",
        "  * A single line or curve separates two classes.\n",
        "* Example:\n",
        "  * Disease detection.\n",
        "  * Loan approval.\n",
        "  * Fraud detection.\n",
        "\n",
        "**2. Softmax Regression**\n",
        "  * Multiclass classification, eg.- Classifying handwritten digits.\n",
        "* Working:\n",
        "  * Extends logistic regression to handle multiple classes using the Softmax function.\n",
        "  * Computes probabilities for all classes, and the class with the highest probability is chosen.\n",
        "* Decision Boundary:\n",
        "  * Separates multiple classes using multiple hyperplanes.\n",
        "* Example:\n",
        "  * Handwritten digit recognition.\n",
        "  * Sentiment analysis.\n",
        "  * Object classification in images.\n",
        "\n",
        "**Logistic Regression vs. Softmax Regression**\n",
        "\n",
        "|Feature\t|Logistic Regression\t|Softmax Regression|\n",
        "|-|||\n",
        "|Problem Type\t|Binary Classification\t|Multiclass Classification |\n",
        "|Output\t|Probability of one class\t|Probability of each class|\n",
        "|Activation Function\t|Sigmoid Function\t|Softmax Function|\n",
        "|Decision Boundary\t|Single hyperplane (linear/non-linear)\t|Multiple hyperplanes |\n",
        "|Final Prediction\t|1 if P(y=1)>0.5, else 0\t|Choose class with highest probability|\n",
        "|Example\t|Spam vs. Not Spam\t|Classifying handwritten digits|"
      ],
      "metadata": {
        "id": "tp9n-TluDSDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "AuBMoszwg-RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "**Ans** - When dealing with multiclass classification, two common approaches are:\n",
        "1. One-vs-Rest or One-vs-All\n",
        "2. Softmax Regression\n",
        "\n",
        "Each has its advantages and is preferred in different scenarios. Let's compare them!\n",
        "\n",
        "1. One-vs-Rest Approach\n",
        "* Working:\n",
        "  * Train one binary classifier per class.\n",
        "  * For a dataset with C classes, train C logistic regression models.\n",
        "  * Each classifier predicts \"Class X vs. All Others\".\n",
        "  * The class with the highest probability wins.\n",
        "* Example (Handwritten Digits 0-9):\n",
        "  * Train 10 models:\n",
        "  * Model 1: \"Is it 0?\" (Yes/No)\n",
        "  * Model 2: \"Is it 1?\" (Yes/No)\n",
        "  * ‚Ä¶\n",
        "  * Model 10: \"Is it 9?\" (Yes/No)\n",
        "* Pick the class with the highest score.\n",
        "\n",
        "**Scikit-learn Implementation:**"
      ],
      "metadata": {
        "id": "PpZeMazQDTb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs')\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "socYGQaxhzFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros:\n",
        "  * Simpler to implement and debug.\n",
        "  * Works well with small datasets.\n",
        "  * Faster for large C (many classes) because it trains C separate models.\n",
        "\n",
        "**2. Softmax Approach**\n",
        "* Working:\n",
        "  * Uses one model to classify all classes together.\n",
        "  * Computes probability for each class using the Softmax function.\n",
        "  * The class with the highest probability wins.\n",
        "\n",
        "**Scikit-learn Implementation:**"
      ],
      "metadata": {
        "id": "e_ZvK5jAh3BQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "oH-Q99_eifY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros:\n",
        "  * More efficient for small C (fewer classes).\n",
        "  * Ensures probabilities sum to 1 (better calibration).\n",
        "  * Single model instead of multiple independent models.\n",
        "\n",
        "**Use OvR vs. Softmax?**\n",
        "\n",
        "|Criteria\t|One-vs-Rest|Softmax|\n",
        "|-|||\n",
        "|Dataset Size\t|Small to Medium\t|Medium to Large|\n",
        "|Number of Classes (C)\t|Large C (eg.- C > 10)\t|Small C (eg.- C ‚â§ 10)|\n",
        "|Computational Cost\t|Cheaper (trains C separate models)\t|More expensive (trains one complex model)|\n",
        "|Interpretability\t|Easier to debug\t|Harder to debug|\n",
        "|Probabilities\t|May not sum to 1\t|Always sum to 1|\n",
        "|Performance\t|Slightly worse if C is small\t|More efficient if C is small|"
      ],
      "metadata": {
        "id": "ZvIfkBrKiiaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 20. How do we interpret coefficients in Logistic Regression?\n",
        "**Ans** - Logistic Regression, the coefficients (Œ≤i) represent the impact of each feature on the probability of the outcome. However, unlike Linear Regression, the relationship is not linear but follows a log-odds transformation using the Sigmoid function.\n",
        "\n",
        "1. The Log-Odds Interpretation\n",
        "The Logistic Regression equation is:\n",
        "\n",
        "        P(Y=1|X) = 1/(1+e‚Åª‚ÅΩŒ≤‚ÇÄ‚Å∫Œ≤‚ÇÅÀ£‚ÇÅ‚Å∫Œ≤‚ÇÇÀ£‚ÇÇ‚Å∫...‚Å∫Œ≤‚ÇôÀ£‚Çô‚Åæ)\n",
        "Taking the logit transformation:\n",
        "\n",
        "        log(P/(1-P)) = Œ≤‚ÇÄ+Œ≤‚ÇÅX‚ÇÅ+Œ≤‚ÇÇX‚ÇÇ+...+Œ≤‚ÇôX‚Çô\n",
        "  * Œ≤‚ÇÄ(Intercept): Log-odds of the outcome when all X's are zero.\n",
        "  * Œ≤·µ¢(Coefficients): Change in log-odds for a one-unit increase in X·µ¢, keeping other variables constant.\n",
        "\n",
        "**2. Interpreting Coefficients with Odds Ratio**\n",
        "\n",
        "Since log-odds are not intuitive, we exponentiate the coefficient to get the Odds Ratio (OR):\n",
        "\n",
        "    OR = eŒ≤·∂¶\n",
        "  * If OR > 1: Feature increases the likelihood of Y=1.\n",
        "  * If OR < 1: Feature decreases the likelihood of Y=1.\n",
        "  * If OR = 1: Feature has no effect.\n",
        "* Example: If Œ≤‚ÇÅ= 0.5, then:\n",
        "\n",
        "          OR = e‚Å∞.‚Åµ ‚âà 1.65\n",
        "This means that a 1-unit increase in X1 makes outcome Y=1 1.65 times more likely.\n",
        "\n",
        "**3. Example: Predicting Disease Risk**\n",
        "\n",
        "Let's say we build a Logistic Regression model to predict heart disease (Y = 1) or no disease (Y = 0) based on age and cholesterol:\n",
        "\n",
        "    log(P/(1‚àíP)) = ‚àí2.5+0.04(Age)+0.02(Cholesterol)\n",
        "\n",
        "|Feature\t|Coefficient\t|Odds Ratio |Interpretation|\n",
        "|-||||\n",
        "|Age\t|0.04\t|1.04\t|Each additional year increases odds of disease by 4%.|\n",
        "|Cholesterol\t|0.02\t|1.02\t|Each unit increase in cholesterol increases odds by 2%.|\n",
        "|Intercept\t|-2.5\t|N/A\t|Base log-odds when Age & Cholesterol = 0.|\n",
        "\n",
        "**4. Interpreting Categorical Features**\n",
        "\n",
        "For binary categorical features (eg.- Male = 1, Female = 0):\n",
        "  * Œ≤ positive ‚Üí That category increases the likelihood of Y=1.\n",
        "  * Œ≤ negative ‚Üí That category decreases the likelihood of Y=1.\n",
        "\n",
        "* Example: If Smoking (1 = Smoker, 0 = Non-Smoker) has Œ≤=1.2\n",
        "Œ≤=1.2, then:\n",
        "\n",
        "      OR = e¬π.¬≤ ‚âà 3.32\n",
        "Smokers are 3.32 times more likely to develop heart disease than non-smokers."
      ],
      "metadata": {
        "id": "N3dq_SyMDU4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "xVfAj3JhDW92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "**Ans** - Python program using Logistic Regression with Scikit-learn"
      ],
      "metadata": {
        "id": "Mt6XKuubDZu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "Xb4HQGF9nsQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "  * Loads a dataset.\n",
        "  * Splits into training (80%) and testing (20%).\n",
        "  * Standardizes the features.\n",
        "  * Fits a Logistic Regression model.\n",
        "  * Makes predictions and evaluates accuracy."
      ],
      "metadata": {
        "id": "FbFlWsyvnwWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
        "**Ans** - Python program that applies L1 regularization in Logistic Regression using penalty='l1' in Scikit-learn."
      ],
      "metadata": {
        "id": "JRhvUezCDbtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L1 Regularization: {accuracy:.2f}\")\n",
        "\n",
        "print(\"Model Coefficients:\", model.coef_)"
      ],
      "metadata": {
        "id": "6_MWzmPioJ_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "  * Loads a dataset.\n",
        "  * Splits into training (80%) and testing (20%).\n",
        "  * Standardizes the features.\n",
        "  * Fits a Logistic Regression model with L1 regularization.\n",
        "  * Uses solver='liblinear'.\n",
        "  * Prints accuracy and model coefficients."
      ],
      "metadata": {
        "id": "a_AWuo05oOjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "**Ans** - Python program that applies L2 regularization in Logistic Regression using penalty='l2' in Scikit-learn."
      ],
      "metadata": {
        "id": "pWY3kuMVDcyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', C=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L2 Regularization: {accuracy:.2f}\")\n",
        "\n",
        "print(\"Model Coefficients:\", model.coef_)"
      ],
      "metadata": {
        "id": "WFzy3JtPolXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "  * Loads a dataset.\n",
        "  * Splits into training (80%) and testing (20%).\n",
        "  * Standardizes the features.\n",
        "  * Fits a Logistic Regression model with L2 regularization.\n",
        "  * Uses solver='lbfgs'.\n",
        "  * Prints accuracy and model coefficients."
      ],
      "metadata": {
        "id": "M3AFLVu2oqXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "**Ans** - Python program that applies Elastic Net Regularization in Logistic Regression using penalty='elasticnet' in Scikit-learn."
      ],
      "metadata": {
        "id": "91lY0PQmDd0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with Elastic Net Regularization: {accuracy:.2f}\")\n",
        "\n",
        "print(\"Model Coefficients:\", model.coef_)"
      ],
      "metadata": {
        "id": "vB7_w2Nno_7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads a dataset.\n",
        "* Splits into training (80%) and testing (20%).\n",
        "* Standardizes the features.\n",
        "* Fits a Logistic Regression model with Elastic Net regularization.\n",
        "* Uses solver='saga'.\n",
        "* Uses l1_ratio=0.5"
      ],
      "metadata": {
        "id": "dDJa-IqPpD2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "**Ans** - Python program that trains a Logistic Regression model for multiclass classification using One-vs-Rest strategy."
      ],
      "metadata": {
        "id": "R6rhgKd8De3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "data = load_digits()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=500)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with One-vs-Rest (OvR): {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "1NVnRCejpoL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Explanation:\n",
        "  * Uses the Digits dataset.\n",
        "  * Splits the data into 80% training and 20% testing.\n",
        "  * Standardizes the features to improve model performance.\n",
        "  * Applies Logistic Regression with multi_class='ovr', meaning:"
      ],
      "metadata": {
        "id": "a0fBHfx7pseE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "**Ans** - Python program that applies GridSearchCV to tune the hyperparameters C and penalty in Logistic Regression using Scikit-learn."
      ],
      "metadata": {
        "id": "gJRLIQ60DjFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Model Accuracy with Best Parameters: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "Px06QaPOqEfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the dataset.\n",
        "* Splits the data into 80% training and 20% testing.\n",
        "* Standardizes the features for better performance.\n",
        "* Defines a parameter grid with:\n",
        "\n",
        "* C (regularization strength: [0.01, 0.1, 1, 10])\n",
        "* penalty.\n",
        "  * Uses GridSearchCV with 5-fold cross-validation to find the best combination.\n",
        "  * Uses solver='liblinear'.\n",
        "  * Finds the best parameters and prints model accuracy."
      ],
      "metadata": {
        "id": "og3NS1gSqIl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy\n",
        "**Ans** - Python program that evaluates Logistic Regression using Stratified K-Fold Cross-Validation and prints the average accuracy."
      ],
      "metadata": {
        "id": "Bg0dnVKkDmJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "model = LogisticRegression()\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
        "\n",
        "print(f\"Accuracy for each fold: {scores}\")\n",
        "print(f\"Average Accuracy: {scores.mean():.2f}\")"
      ],
      "metadata": {
        "id": "j_LsLDHtrAIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads a dataset.\n",
        "* Standardizes the features.\n",
        "* Uses StratifiedKFold (K=5) to ensure class distribution is balanced in each fold.\n",
        "* Evaluates Logistic Regression using cross_val_score().\n",
        "* Prints accuracy for each fold and the average accuracy."
      ],
      "metadata": {
        "id": "GMidcjRgrDpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "**Ans** - Python program that loads a dataset from a CSV file, applies Logistic Regression, and evaluates its accuracy."
      ],
      "metadata": {
        "id": "OoxGc6RNDneN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "\n",
        "X = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "IT0LeKqIrYW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads a dataset from a CSV file (data.csv).\n",
        "* Splits the data into features (X) and target (y).\n",
        "* Divides data into training (80%) and testing (20%).\n",
        "* Standardizes the features for better performance.\n",
        "* Trains a Logistic Regression model on the training data.\n",
        "* Predicts the test set and calculates accuracy."
      ],
      "metadata": {
        "id": "NgTZxp-PrcIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "**Ans** - Python program that applies RandomizedSearchCV to tune the hyperparameters C, penalty, and solver for Logistic Regression.\n"
      ],
      "metadata": {
        "id": "IZDei_klDobN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "param_dist = {\n",
        "    'C': uniform(0.01, 10),\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(LogisticRegression(), param_distributions=param_dist,\n",
        "                                   n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Model Accuracy with Best Parameters: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "dlIHXIAQrw8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads a dataset.\n",
        "* Splits the data into 80% training and 20% testing.\n",
        "* Standardizes the features for better performance."
      ],
      "metadata": {
        "id": "9YrxfddPr1hS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "**Ans** - Python program that implements One-vs-One Multiclass Logistic Regression and prints the accuracy."
      ],
      "metadata": {
        "id": "1XGfafaYDpV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "data = load_digits()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = OneVsOneClassifier(LogisticRegression())\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with One-vs-One (OvO): {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "1tlpdba6tjfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Digits dataset.\n",
        "* Splits data into 80% training and 20% testing.\n",
        "* Standardizes features for better performance.\n",
        "* Uses OneVsOneClassifier for OvO classification.\n",
        "* Trains Logistic Regression in a One-vs-One setting:\n",
        "\n",
        "* Trains a separate model for each pair of classes.\n",
        "* Uses a voting mechanism to classify new samples.\n",
        "  * Prints model accuracy on test data."
      ],
      "metadata": {
        "id": "xidjdLlEtocm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "**Ans** - Python program that trains a Logistic Regression model and visualizes the confusion matrix for a binary classification problem using seaborn and matplotlib."
      ],
      "metadata": {
        "id": "wC2E2lpqDqOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mCKBFI2juCpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads a binary classification dataset.\n",
        "* Splits the data into 80% training and 20% testing.\n",
        "* Standardizes features for better performance.\n",
        "* Trains a Logistic Regression model on the training data.\n",
        "* Predicts labels on the test set and computes accuracy.\n",
        "* Generates a confusion matrix using confusion_matrix(y_test, y_pred).\n",
        "* Visualizes the confusion matrix using seaborn.heatmap()."
      ],
      "metadata": {
        "id": "sIRu1l1TuG40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "**Ans** - Python program that trains a Logistic Regression model and evaluates its performance using Precision, Recall, and F1-Score for a binary classification problem."
      ],
      "metadata": {
        "id": "Oh7wo_I5Dq-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "QIg6RE1GunW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads a binary classification dataset.\n",
        "* Splits the data into 80% training and 20% testing.\n",
        "* Standardizes features for better performance.\n",
        "* Trains a Logistic Regression model on the training data.\n",
        "* Predicts labels on the test set."
      ],
      "metadata": {
        "id": "L1zhg1oHurtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "**Ans** - Python program that trains a Logistic Regression model on an imbalanced dataset and applies class weights to improve performance."
      ],
      "metadata": {
        "id": "EfEv60TeDroN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.datasets import make_classification\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "X, y = make_classification(n_samples=5000, n_features=10, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression(class_weight='balanced', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "\n",
        "cm = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0PJC-bUEvD4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Generates an imbalanced dataset.\n",
        "* Splits the data into 80% training and 20% testing.\n",
        "* Standardizes features for better model performance.\n",
        "* Uses class_weight='balanced' to give higher weight to the minority class.\n",
        "* Trains Logistic Regression and makes predictions.\n",
        "* Evaluates performance using Precision, Recall, and F1-Score.\n",
        "* Visualizes the confusion matrix using seaborn.heatmap()."
      ],
      "metadata": {
        "id": "-h_mrZS2vJxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "**Ans** - Python program that trains Logistic Regression on the Titanic dataset, handles missing values, and evaluates model performance using accuracy, precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "-xZ9EStIDsb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
        "target = \"Survived\"\n",
        "df = df[features + [target]]\n",
        "\n",
        "df[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\n",
        "df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0], inplace=True)\n",
        "\n",
        "categorical_features = [\"Sex\", \"Embarked\"]\n",
        "numerical_features = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "cat_pipeline = Pipeline([\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", num_pipeline, numerical_features),\n",
        "    (\"cat\", cat_pipeline, categorical_features)\n",
        "])\n",
        "\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = Pipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", LogisticRegression())\n",
        "])\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Not Survived\", \"Survived\"], yticklabels=[\"Not Survived\", \"Survived\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RhUy89tIvgts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Titanic dataset directly from GitHub.\n",
        "* Handles missing values in Age and Embarked.\n",
        "* Prepares data by encoding categorical variables and scaling numerical features.\n",
        "* Splits the dataset into 80% training and 20% testing.\n",
        "* Uses a preprocessing pipeline to simplify transformations.\n",
        "* Trains Logistic Regression using the preprocessed data.\n",
        "* Computes Accuracy, Precision, Recall, and F1-Score for evaluation.\n",
        "* Visualizes the confusion matrix using seaborn.heatmap()."
      ],
      "metadata": {
        "id": "H7hTqUMvvqwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "**Ans** - Python program that trains Logistic Regression with and without feature scaling, evaluates accuracy, and compares results."
      ],
      "metadata": {
        "id": "MtvKb-CLDtVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model_no_scaling = LogisticRegression()\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_with_scaling = LogisticRegression()\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "print(f\"Accuracy without Scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with Scaling: {accuracy_with_scaling:.4f}\")\n",
        "\n",
        "if accuracy_with_scaling > accuracy_no_scaling:\n",
        "    print(\"Feature scaling improved accuracy!\")\n",
        "else:\n",
        "    print(\"Feature scaling did not improve accuracy significantly.\")"
      ],
      "metadata": {
        "id": "9b3_98V2wDOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Breast Cancer dataset.\n",
        "* Splits data into 80% training and 20% testing.\n",
        "* Trains Logistic Regression without feature scaling and records accuracy.\n",
        "* Applies Standardization to transform features to zero mean and unit variance.\n",
        "* Trains Logistic Regression again with scaled data and records accuracy.\n",
        "* Compares accuracy before and after scaling to see the impact."
      ],
      "metadata": {
        "id": "H3j7vlX9wHyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "**Ans** - Python program that trains a Logistic Regression model and evaluates its performance using the ROC-AUC score."
      ],
      "metadata": {
        "id": "rRCSP6a1DuD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")"
      ],
      "metadata": {
        "id": "_arDawIIwgcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Breast Cancer dataset.\n",
        "* Splits data into 80% training and 20% testing.\n",
        "* Applies Standardization to improve model performance.\n",
        "* Trains Logistic Regression and predicts probabilities for class 1.\n",
        "* Computes the ROC-AUC score to measure model performance.\n",
        "* Plots the ROC curve to visualize the True Positive Rate vs. False Positive Rate.\n",
        "* Displays the AUC  score, which indicates how well the model distinguishes between classes.\n",
        "\n",
        "*  AUC close to 1 = Excellent model, AUC ‚âà 0.5 = Random guess!"
      ],
      "metadata": {
        "id": "VigMu6uTwljA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "**Ans** - Python program that trains a Logistic Regression model using a custom learning rate (C=0.5) and evaluates accuracy."
      ],
      "metadata": {
        "id": "zQSnqhG3Du2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression(C=0.5, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy with C=0.5: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "LlNvGlTfw51q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Breast Cancer dataset.\n",
        "* Splits data into 80% training and 20% testing.\n",
        "* Applies Standardization for better model performance.\n",
        "* Trains Logistic Regression with a custom inverse regularization strength C = 0.5.\n",
        "* Evaluates accuracy to measure model performance.\n",
        "\n",
        "**C=0.5 mean**\n",
        "* C is the inverse of the regularization strength.\n",
        "* Lower C (e.g., C=0.1) ‚Üí More regularization ‚Üí Prevents overfitting but may underfit.\n",
        "* Higher C (e.g., C=10) ‚Üí Less regularization ‚Üí More flexible model, but may overfit."
      ],
      "metadata": {
        "id": "F_PdgGWvw-Nc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 18. Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "**Ans** - Python program that trains a Logistic Regression model and identifies important features based on model coefficients."
      ],
      "metadata": {
        "id": "aNIE43QIDvmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "feature_importance = np.abs(model.coef_[0])\n",
        "\n",
        "feature_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
        "feature_df = feature_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"Top 10 Important Features:\")\n",
        "print(feature_df.head(10))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_df['Feature'][:10], feature_df['Importance'][:10], color='blue')\n",
        "plt.xlabel(\"Coefficient Magnitude\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Top 10 Important Features in Logistic Regression\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AbSahhZDxWR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Breast Cancer dataset.\n",
        "* Splits data into 80% training and 20% testing.\n",
        "* Applies Standardization for better model performance.\n",
        "* Trains Logistic Regression on scaled data.\n",
        "* Extracts feature importance using absolute values of model coefficients.\n",
        "* Sorts features by importance and prints the top 10.\n",
        "* Visualizes the top 10 important features using a bar chart.\n",
        "\n",
        "**Use of Model Coefficients**\n",
        "* Higher absolute coefficient values ‚Üí More influence on predictions.\n",
        "* Negative vs. Positive Coefficients\n",
        "  * Positive: Increases probability of the positive class.\n",
        "  * Negative: Decreases probability of the positive class.\n",
        "* Feature selection based on important coefficients helps reduce dimensionality and improve model efficiency."
      ],
      "metadata": {
        "id": "TE_0X9N3xc19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen's Kappa Score.\n",
        "**Ans** - Python program that trains a Logistic Regression model and evaluates its performance using Cohen's Kappa Score."
      ],
      "metadata": {
        "id": "KMi2H0tjDwTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.4f}\")"
      ],
      "metadata": {
        "id": "cS3BeObjzuJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Breast Cancer dataset.\n",
        "* Splits data into 80% training and 20% testing.\n",
        "* Applies Standardization for better model performance.\n",
        "* Trains Logistic Regression on scaled data.\n",
        "* Computes Accuracy to measure overall correctness.\n",
        "* Computes Cohen's Kappa Score to evaluate agreement beyond random chance.\n",
        "\n",
        "**Cohen's Kappa Score**\n",
        "* Measures classification agreement between predicted and actual labels.\n",
        "* Corrects for chance agreement, unlike accuracy.\n",
        "* Formula:\n",
        "\n",
        "      K = (p‚Çí-p‚Çë)/(1-p‚Çë)\n",
        "* p‚Çí = observed agreement.\n",
        "* p‚Çë = expected agreement by chance.\n",
        "\n",
        "**Interpretation of Kappa Score:**\n",
        "\n",
        "|Kappa Score\t|Interpretation|\n",
        "|-||\n",
        "|< 0\t|No agreement|\n",
        "|0 - 0.20\t|Slight agreement|\n",
        "|0.21 - 0.40\t|Fair agreement|\n",
        "|0.41 - 0.60\t|Moderate agreement|\n",
        "|0.61 - 0.80\t|Substantial agreement|\n",
        "|0.81 - 1.00\t|Almost perfect agreement|"
      ],
      "metadata": {
        "id": "E_EaOdRzzzB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "**Ans** - Python program that trains a Logistic Regression model and visualizes the Precision-Recall Curve for binary classification."
      ],
      "metadata": {
        "id": "mpmlGGgyDxNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "avg_precision = average_precision_score(y_test, y_prob)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(recall, precision, color='blue', label=f'PR Curve (AP = {avg_precision:.2f})')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Average Precision Score: {avg_precision:.4f}\")"
      ],
      "metadata": {
        "id": "-OHmhaz71YVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Breast Cancer dataset.\n",
        "* Splits data into 80% training and 20% testing.\n",
        "* Applies Standardization for better model performance.\n",
        "* Trains Logistic Regression on scaled data.\n",
        "* Computes Precision-Recall Curve to evaluate model performance.\n",
        "* Calculates Average Precision Score, which is the area under the Precision-Recall curve.\n",
        "* Plots the Precision-Recall curve using matplotlib.\n",
        "\n",
        "**Use of Precision-Recall Curve**\n",
        "* Useful for imbalanced datasets where accuracy can be misleading.\n",
        "* Better than ROC Curve when dealing with rare positive classes.\n",
        "* Measures trade-off between Precision and Recall.\n",
        "\n",
        "**Interpretation of Precision-Recall Curve:**\n",
        "* Higher Precision & Recall = Better Model\n",
        "* Curve closer to (1,1) = Strong classifier\n",
        "* AP Score close to 1 = Excellent performance"
      ],
      "metadata": {
        "id": "QV2K1XCX1cv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "**Ans** - Python program that trains a Logistic Regression model using different solvers and compares their accuracy."
      ],
      "metadata": {
        "id": "OhNmD0UDDyKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "solvers = [\"liblinear\", \"saga\", \"lbfgs\"]\n",
        "accuracy_results = {}\n",
        "\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=500, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_results[solver] = accuracy\n",
        "    print(f\"Accuracy with solver '{solver}': {accuracy:.4f}\")\n",
        "\n",
        "accuracy_df = pd.DataFrame(list(accuracy_results.items()), columns=[\"Solver\", \"Accuracy\"])\n",
        "print(\"\\nComparison of Solvers:\")\n",
        "print(accuracy_df)"
      ],
      "metadata": {
        "id": "z98hqEoE17dY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Breast Cancer dataset.\n",
        "* Splits data into 80% training and 20% testing.\n",
        "* Applies Standardization for better model performance.\n",
        "* Trains Logistic Regression using three different solvers:\n",
        "  * liblinear ‚Üí Works well for small datasets, supports L1 & L2 regularization.\n",
        "  * saga ‚Üí Best for large datasets, supports L1, L2, and Elastic Net regularization.\n",
        "  * lbfgs ‚Üí Good for medium-sized datasets, supports only L2 regularization.\n",
        "  * Evaluates accuracy for each solver and prints the results.\n",
        "\n",
        "**Use of Solver**\n",
        "\n",
        "|Solver\t|Pros\t|Best Use Cases|\n",
        "|-|||\n",
        "|liblinear\t|Good for small datasets, supports L1 & L2\t|Binary classification, small datasets|\n",
        "|saga\t|Works well for large datasets, supports L1, L2, ElasticNet\t|Large-scale data, sparse datasets|\n",
        "|lbfgs\t|Default for multi-class problems, supports only L2\t|Medium-sized datasets, multi-class|\n",
        "\n",
        "**Expected Output Example:**"
      ],
      "metadata": {
        "id": "9-JpMULi1_cL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy with solver 'liblinear': 0.9649\n",
        "Accuracy with solver 'saga': 0.9649\n",
        "Accuracy with solver 'lbfgs': 0.9649"
      ],
      "metadata": {
        "id": "riA3XaJw5KrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "**Ans** - Python program that trains a Logistic Regression model and evaluates its performance using Matthews Correlation Coefficient."
      ],
      "metadata": {
        "id": "pHogWZG4Dy2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef, accuracy_score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "mcc_score = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc_score:.4f}\")"
      ],
      "metadata": {
        "id": "u-FRF5G_6bnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Breast Cancer dataset.\n",
        "* Splits data into 80% training and 20% testing.\n",
        "* Applies Standardization for better model performance.\n",
        "* Trains Logistic Regression on scaled data.\n",
        "* Computes Accuracy to measure overall correctness.\n",
        "* Computes Matthews Correlation Coefficient for a balanced evaluation of classification performance.\n",
        "\n",
        "**Matthews Correlation Coefficient**\n",
        "* Measures classification quality, especially for imbalanced datasets.\n",
        "* Formula:\n",
        "\n",
        "      MCC = (TPxTN-FPxFN)/‚àö(TP+FP)(TP+FN)(TN+FP)(TN+FN)\n",
        "  * TP = True Positives\n",
        "  * TN = True Negatives\n",
        "  * FP = False Positives\n",
        "  * FN = False Negatives\n",
        "\n",
        "**Interpretation of MCC Score:**\n",
        "\n",
        "|MCC Score\t|Interpretation|\n",
        "|-||\n",
        "|+1\t|Perfect prediction|\n",
        "|0\t|No better than random guessing|\n",
        "|-1\t|Completely wrong predictions|"
      ],
      "metadata": {
        "id": "Jk_95IuK6ft2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "**Ans** - Python program that trains Logistic Regression on both raw and standardized data and compares their accuracy to see the impact of feature scaling."
      ],
      "metadata": {
        "id": "tzES8noGDzdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model_raw = LogisticRegression(max_iter=500, random_state=42)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=500, random_state=42)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy on Raw Data: {accuracy_raw:.4f}\")\n",
        "print(f\"Accuracy on Standardized Data: {accuracy_scaled:.4f}\")\n",
        "\n",
        "if accuracy_scaled > accuracy_raw:\n",
        "    print(\"\\nFeature scaling improved the model's performance!\")\n",
        "elif accuracy_scaled < accuracy_raw:\n",
        "    print(\"\\nRaw data performed better, but this is unusual.\")\n",
        "else:\n",
        "    print(\"\\nFeature scaling had no impact, likely because all features were already on similar scales.\")"
      ],
      "metadata": {
        "id": "OXMX0Fz_7kAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Breast Cancer dataset.\n",
        "* Splits data into 80% training and 20% testing.\n",
        "* Trains Logistic Regression on raw data and computes accuracy.\n",
        "* Applies Standardization to scale the features.\n",
        "* Trains Logistic Regression on standardized data and computes accuracy.\n",
        "* Compares the accuracy of both models to see the impact of feature scaling.\n",
        "\n",
        "**Expected Output (Example):**"
      ],
      "metadata": {
        "id": "lOx1iETJ7oRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy on Raw Data: 0.9474\n",
        "Accuracy on Standardized Data: 0.9649\n",
        "\n",
        "Feature scaling improved the model's performance!"
      ],
      "metadata": {
        "id": "K6FP0h-T8CHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "**Ans** - Python program that trains Logistic Regression and finds the optimal C using cross-validation."
      ],
      "metadata": {
        "id": "A5pkGnUzD1IW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "C_values = np.logspace(-3, 3, 10)\n",
        "cv_scores = []\n",
        "\n",
        "for C in C_values:\n",
        "    model = LogisticRegression(C=C, max_iter=500, random_state=42)\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    cv_scores.append(scores.mean())\n",
        "\n",
        "optimal_C = C_values[np.argmax(cv_scores)]\n",
        "\n",
        "best_model = LogisticRegression(C=optimal_C, max_iter=500, random_state=42)\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "test_accuracy = best_model.score(X_test, y_test)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.semilogx(C_values, cv_scores, marker='o', linestyle='dashed', color='b')\n",
        "plt.xlabel(\"C (Regularization Strength)\")\n",
        "plt.ylabel(\"Cross-Validation Accuracy\")\n",
        "plt.title(\"Optimal C Selection for Logistic Regression\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Optimal C: {optimal_C:.4f}\")\n",
        "print(f\"Test Accuracy with Optimal C: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "R-_bi1ip8R4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Breast Cancer dataset.\n",
        "* Splits data into 80% training and 20% testing.\n",
        "* Applies Standardization for better model performance.\n",
        "* Defines a range of C values.\n",
        "* Performs 5-fold Cross-Validation for each C to find the best one.\n",
        "* Finds the optimal C value.\n",
        "* Trains the final model using the optimal C and evaluates on test data.\n",
        "* Plots C values vs. cross-validation accuracy for visualization.\n",
        "\n",
        "**Expected Output (Example):**"
      ],
      "metadata": {
        "id": "EYCakA2W8Xby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Optimal C: 0.4642\n",
        "Test Accuracy with Optimal C: 0.9737"
      ],
      "metadata": {
        "id": "b_6Ssoy_8sYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "**Ans** - Python program that trains a Logistic Regression model, saves the trained model using joblib, and then loads it again to make predictions."
      ],
      "metadata": {
        "id": "t4wifbybD2i9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression(max_iter=500, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "joblib.dump(model, \"logistic_regression_model.pkl\")\n",
        "\n",
        "loaded_model = joblib.load(\"logistic_regression_model.pkl\")\n",
        "\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(\"Logistic Regression model has been saved and loaded successfully!\")"
      ],
      "metadata": {
        "id": "1Ny0OtdG86OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Breast Cancer dataset.\n",
        "* Splits data into 80% training and 20% testing.\n",
        "* Applies Standardization for better model performance.\n",
        "* Trains a Logistic Regression model on scaled data.\n",
        "* Saves the trained model using joblib.dump() to \"logistic_regression_model.pkl\".\n",
        "* Loads the saved model using joblib.load().\n",
        "* Makes predictions using the loaded model to verify correctness.\n",
        "* Computes accuracy to ensure the loaded model works as expected.\n",
        "\n",
        "**Expected Output Example**"
      ],
      "metadata": {
        "id": "AsEP4L0B8-Zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model Accuracy: 0.9649\n",
        "Logistic Regression model has been saved and loaded successfully!"
      ],
      "metadata": {
        "id": "w-niK5pw9TWC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}